{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b83e32b-7db1-411a-bfea-fa5dad0aa022",
   "metadata": {},
   "source": [
    "# **Titanic - Machine Learning from Disaster**\n",
    "\n",
    "### **Steps**\n",
    "1. Import and profile data\n",
    "2. Transform categorical variables\n",
    "3. Correlation Matrix\n",
    "4. Feature Engineering, Impute, Bin Features\n",
    "5. Add Interactions\n",
    "6. Score Models\n",
    "7. Tune Hyper Parameters - if necessary\n",
    "8. Calculate Model Probabilities\n",
    "9. Create Ensemble Models\n",
    "10. Calculate Model Performance Metrics\n",
    "11. Select Final Model\n",
    "12. Submit to Kaggle\n",
    "13. Compare Public Score with Leaderboard\n",
    "\n",
    "### **Workflow**\n",
    "First, **iterate** thru steps 1, 2, 3, 4, and 6 to determine:\n",
    "1. To hot-encode or not.\n",
    "2. To bin features or not.\n",
    "3. Use fare or do a log transformation.\n",
    "4. Best way to impute missing features.\n",
    "5. Include Family Size or not.\n",
    "6. How to group tickets and titles.\n",
    "\n",
    "Once enough progress was made then turned to tuning the hyper parameters by **running** steps 1, 2, 3, 4, 6, 7, 8, and 10.\n",
    "**SVC** emerged as the best model while the Random Forest, Decision Tree, and XGBoost all showed evidence of overfitting.\n",
    "\n",
    "Decided to **store models in a dictionary** to make it easier to **iterate** thru models and to explore if some models \n",
    "worked better with scaled features or not.\n",
    "\n",
    "Next, decided to explore **feature interactions (step 5)** and **iterated** thru steps 1, 2, 3, 4, 5, 6, 7, 8, and 10 to \n",
    "determine which interactions to include. Decided to focus on the features with highest importance based on the \n",
    "findings from **step 7 (hyper-parameter).**\n",
    "\n",
    "Finally, observed that SVC is good at predicting true non survivors, but other models good at predicting survivors, \n",
    "so decided to build several ensemble models and **running** steps 1 thru 10.\n",
    "\n",
    "**Final** model selected was and **Ensemble Model** made up of SVC and a Random Forest Model with tuned hyper-parameters.\n",
    "\n",
    "### **Findings**\n",
    "1. Categorical features performed better if they are one-hot encoded.\n",
    "2. Continuous features like Fare, Age, and Family Size performed better as continuous instead of as bins.\n",
    "3. Doing this results in an above median-model with a Public Score of **0.79186**.\n",
    "4. Adding feature interactions provided gains, but the impact was minimal.\n",
    "5. No need to iterate over hyper-parameters for further gains. Run study and settle on best parameters.\n",
    "6. Adding ensemble models resulted in big improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c609e9dd-8fe7-494d-952a-13bc5627dcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pandas as pd, numpy as np, random, seaborn as sns, matplotlib.pyplot as plt, warnings, re, itertools\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "plot_title_dict = {'fontsize': 18, 'fontweight': 'bold'}\n",
    "plot_axis_dict = {'fontsize': 12, 'fontweight': 'bold'}\n",
    "\n",
    "# Function to plot survival rate by feature\n",
    "def surv_percent_fun(col_name, fig_size):    \n",
    "    counts_df = train_df\\\n",
    "        .groupby([col_name], as_index = False)\\\n",
    "        .agg(Count = (col_name, 'count'))\n",
    "\n",
    "    surv_pct_df = train_df\\\n",
    "        .groupby([col_name, 'Survived'], as_index = False)\\\n",
    "        .agg(Count = (col_name, 'count'))\\\n",
    "        .pivot(index = col_name, columns = 'Survived', values = 'Count')\\\n",
    "        .fillna(0)\n",
    "    surv_pct_df['Total'] = surv_pct_df[0] + surv_pct_df[1]\n",
    "    surv_pct_df[0] = surv_pct_df[0] / surv_pct_df['Total']\n",
    "    surv_pct_df[1] = surv_pct_df[1] / surv_pct_df['Total']\n",
    "    del surv_pct_df['Total']\n",
    "    surv_pct_df = surv_pct_df[surv_pct_df.columns[::-1]]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = fig_size)\n",
    "    surv_pct_df.plot(kind = 'bar', stacked = True, color = ['lightblue', 'grey'], ax = ax)\n",
    "    plt.title('Survival Rates by ' + col_name.replace('_', ' '), fontdict = plot_title_dict)\n",
    "    plt.ylabel('Survival Rate', fontdict = plot_axis_dict)\n",
    "    plt.xlabel(col_name.replace('_', ' ') + ' (Count)', fontdict = plot_axis_dict)\n",
    "    ax.set_xticklabels([str(t) + ' (' + str(c) + ')' for t, c in zip(counts_df[col_name], counts_df['Count'])])\n",
    "    ax.tick_params(axis = 'x', labelrotation = 0)\n",
    "    ax.yaxis.set_major_formatter(mtick.PercentFormatter(xmax = 1))\n",
    "    plt.grid(axis = 'y')\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6cac17-671a-4e6b-af3c-e3d1ab7961a4",
   "metadata": {},
   "source": [
    "# 1. Import and profile data\n",
    "\n",
    "4 features have missing values: Cabin (1,014), Age (263), Embarked (2), Fare (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0d5fcd2-7958-48e5-9f55-edb91e4349f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('/kaggle/input') == False:\n",
    "    kaggle_path = './kaggle/input'\n",
    "else:\n",
    "    kaggle_path = '/kaggle/input'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bb2a5a6-c201-42a7-b1ed-1a6b6e6d0ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(kaggle_path + '/titanic/train.csv')\n",
    "test_df = pd.read_csv(kaggle_path + '/titanic/test.csv')\n",
    "df_list = [train_df, test_df]\n",
    "plot_df = train_df.copy()[['Survived', 'Age', 'Embarked', 'Pclass', 'Sex', 'SibSp', 'Parch']]\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261afd01-6566-4d73-b02a-2f920951ce1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in df_list:\n",
    "    df.info()\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a804ca7-f04c-4e3c-9f53-b4222447d8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565627d6-2374-4e32-837b-ff3acd44bc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe(include = ['O'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3d4f0d-7684-4c51-b50e-0eb09a063174",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([train_df, test_df]).isnull().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0c883f-c6cf-4d74-9fa0-16297b188c98",
   "metadata": {},
   "source": [
    "# 2. Transform Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0cf85b-0c64-4be9-8231-37c2bf7d7c4b",
   "metadata": {},
   "source": [
    "# 2.1 Ticket\n",
    "\n",
    "1. Create ticket group based on alphabetic characters in ticket\n",
    "2. Map groups only if ticket characters are in both sets\n",
    "3. One hot encode ticket group\n",
    "4. Feature Engineering: ticket frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a115bd-32f3-40df-beff-d202a26e9ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickets_dict = {'A': ['A', 'A/5', 'A5', 'A/4', 'A4', 'A/S', 'AQ/3', 'AQ/4'],\n",
    "                'C': ['CA', 'C'],\n",
    "                'P': ['PC', 'PP', 'P/PP', 'SW/PP'],\n",
    "                'SC': ['SC/PARIS', 'SC/AH', 'SC/A4', 'SC/AH BASLE', 'SC/A3', 'SCO/W'], \n",
    "                'SO': ['SOC', 'SO/PP', 'SO/C', 'SOP', 'SP'], \n",
    "                'Soton': ['SOTON/OQ', 'SOTON/O2', 'CA/SOTON'], \n",
    "                'Ston': ['STON/O', 'STON/O2', 'STON/OQ'], \n",
    "                'W': ['W/C', 'WE/P', 'WEP']}\n",
    "tickets_alpha = list(itertools.chain(*[v for v in tickets_dict.values()]))\n",
    "\n",
    "for df in df_list:\n",
    "    df['TicketIsNumeric'] = [t.isnumeric() for t in df['Ticket']]    \n",
    "    df['Ticket_Alpha'] = [t.upper().replace('.', '') for t in df['Ticket']]\n",
    "    df['Ticket_Alpha'] = [re.sub(r' \\d+', '', t) for t in df['Ticket_Alpha']]\n",
    "    df['Ticket_Alpha_Group'] = ['Other' if t not in tickets_alpha else t for t in df['Ticket_Alpha']]\n",
    "    df['Ticket_Group'] = 'Missing'\n",
    "    df.loc[df['TicketIsNumeric'], 'Ticket_Group'] = 'Numeric'\n",
    "    df.loc[df['Ticket_Group'] == 'Missing', 'Ticket_Group'] = df['Ticket_Alpha_Group']\n",
    "    for d in tickets_dict:\n",
    "        df['Ticket_Group'] = df['Ticket_Group'].replace(tickets_dict[d], d) \n",
    "    df.drop(['Ticket_Alpha_Group'], axis = 1, inplace = True)\n",
    "\n",
    "surv_percent_fun('Ticket_Group', (14, 6))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5356c002-7b58-4d88-9faf-59987c0a68ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticket_train_df = train_df[train_df['TicketIsNumeric'] == False]\\\n",
    "    .groupby(['Ticket_Alpha', 'Ticket_Group'], as_index = False)\\\n",
    "    .agg(Count = ('Ticket_Alpha', 'count'))\n",
    "ticket_train_df['Set'] = 'Train'\n",
    "\n",
    "ticket_test_df = test_df[test_df['TicketIsNumeric'] == False]\\\n",
    "    .groupby(['Ticket_Alpha', 'Ticket_Group'], as_index = False)\\\n",
    "    .agg(Count = ('Ticket_Alpha', 'count'))\n",
    "ticket_test_df['Set'] = 'Test'\n",
    "\n",
    "pd.concat([ticket_train_df, ticket_test_df])\\\n",
    "    .pivot(index = ['Ticket_Group', 'Ticket_Alpha'], columns = 'Set', values = 'Count')\\\n",
    "    .sort_values(by = ['Ticket_Group', 'Train'], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be4a858-5255-4011-9712-f212a55794e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df['Ticket_Group'] = train_df['Ticket_Group']\n",
    "train_df = pd.get_dummies(train_df, columns = ['Ticket_Group'], drop_first = False)\n",
    "test_df = pd.get_dummies(test_df, columns = ['Ticket_Group'], drop_first = False)\n",
    "df_list = [train_df, test_df]\n",
    "\n",
    "for df in df_list:\n",
    "    df.drop(['TicketIsNumeric', 'Ticket_Alpha'], axis = 1, inplace = True)    \n",
    "    for c in [c for c in df.columns if 'Ticket_Group' in c]:\n",
    "        df[c] = df[c].astype(int)\n",
    "    df.columns = df.columns.str.replace('Ticket_Group', 'Ticket')\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dd72f2-f5ca-4b35-9960-ee286656e412",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticket_freq_df = pd.concat([train_df, test_df])\\\n",
    "        .groupby(['Ticket'], as_index = False)\\\n",
    "        .agg(Ticket_Freq = ('Ticket', 'count'))\n",
    "\n",
    "for df in df_list:\n",
    "    df['Ticket_Freq'] = df.merge(ticket_freq_df, on = ['Ticket'], how = 'left')['Ticket_Freq']\n",
    "\n",
    "plot_df['Ticket_Freq'] = train_df['Ticket_Freq']\n",
    "\n",
    "surv_percent_fun('Ticket_Freq', (14, 6))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e242b4bc-dc6e-48ee-8cee-6921cb1c9a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in df_list:\n",
    "    df.drop(['Ticket'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0f8c95-a067-4efd-b048-01da74fa7e3a",
   "metadata": {},
   "source": [
    "# 2.2 Name\n",
    "1. Extract title from Name\n",
    "2. Map titles\n",
    "3. One hot encode title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1afc1b-dde5-46ff-94cf-622b94d8a9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in df_list:\n",
    "    df['Title'] = df['Name'].str.extract(r'([A-Za-z]+)\\.', expand = False)\n",
    "    df.drop(['Name'], axis = 1, inplace = True)\n",
    "\n",
    "pd.crosstab(train_df['Title'], train_df['Sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a08d66-59c5-4a21-bf91-bb396d396039",
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_titles = ['Lady', 'Countess', 'Capt', 'Col', 'Don', 'Major', 'Sir', 'Jonkheer', 'Dona']\n",
    "\n",
    "for df in df_list:\n",
    "    df['Title'] = df['Title'].replace(rare_titles, 'Rare')\n",
    "    df['Title'] = df['Title'].replace('Mlle', 'Miss')\n",
    "    df['Title'] = df['Title'].replace('Ms', 'Miss')\n",
    "    df['Title'] = df['Title'].replace('Mme', 'Mrs')\n",
    "plot_df['Title'] = train_df['Title'] \n",
    "\n",
    "surv_percent_fun('Title', (14, 6))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28afb5f2-fbaf-4ee3-9470-7d398cf32897",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df['Title'] = train_df['Title']\n",
    "train_df = pd.get_dummies(train_df, columns = ['Title'], drop_first = False)\n",
    "test_df = pd.get_dummies(test_df, columns = ['Title'], drop_first = False)\n",
    "df_list = [train_df, test_df]\n",
    "\n",
    "for df in df_list:\n",
    "    for c in [c for c in df.columns if 'Title' in c]:\n",
    "        df[c] = df[c].astype(int)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa1dd5b-44b7-4658-900f-2c6fe5386904",
   "metadata": {},
   "source": [
    "# 2.2 Sex\n",
    "1. One hot encode sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3673f7d-3de5-4e88-9154-46541b7e486c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in df_list:\n",
    "    df['Sex'] = df['Sex'].str.title()\n",
    "\n",
    "surv_percent_fun('Sex', (14, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff206c06-a835-4aaf-afbe-1f6180de91f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df['Sex'] = train_df['Sex']\n",
    "train_df = pd.get_dummies(train_df, columns = ['Sex'], drop_first = False)\n",
    "test_df = pd.get_dummies(test_df, columns = ['Sex'], drop_first = False)\n",
    "df_list = [train_df, test_df]\n",
    "\n",
    "for df in df_list:\n",
    "    for c in [c for c in df.columns if 'Sex' in c]:\n",
    "        df[c] = df[c].astype(int)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b812305c-b68e-4d7f-967f-a77cfb80a995",
   "metadata": {},
   "source": [
    "# 2.3 Port of Embarkation\n",
    "1. Impute missing with most frequent\n",
    "2. Add Missing Port Indicator\n",
    "3. One hot encode Embarked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8748507e-3581-4428-a69e-f9eb1e903776",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_port = train_df.Embarked.dropna().mode()[0]\n",
    "print('Most frequent port is ' + freq_port)\n",
    "\n",
    "for df in df_list:\n",
    "    df.loc[df['Embarked'].isnull(), 'Embarked_Missing'] = 1\n",
    "    df['Embarked_Missing'] = df['Embarked_Missing'].fillna(0).astype(int)\n",
    "    df['Embarked'] = df['Embarked'].fillna(freq_port)\n",
    "\n",
    "surv_percent_fun('Embarked', (14, 6))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a368d06-b2f1-4b5a-8227-1447a43fd565",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey = True, figsize = (14, 6))\n",
    "\n",
    "for poe, ax in zip(['S', 'C', 'Q'], [ax1, ax2, ax3]):\n",
    "    sns.lineplot(x = 'Pclass', y = 'Survived', hue = 'Sex',\n",
    "                 data = plot_df[plot_df['Embarked'] == poe], ax = ax)\n",
    "    ax.set_xlabel('Pclass', fontdict = plot_axis_dict)\n",
    "    ax.set_title(\"Embarked = \" + poe, fontdict = plot_axis_dict)\n",
    "    ax.locator_params(axis = 'x', nbins = 3)\n",
    "ax1.set_ylabel('Survival Rate', fontdict = plot_axis_dict)\n",
    "ax1.yaxis.set_major_formatter(mtick.PercentFormatter(xmax = 1))\n",
    "plt.suptitle('Survival Rates by Port of Embarkation and Class', \n",
    "             fontsize = plot_title_dict['fontsize'],\n",
    "             fontweight = plot_title_dict['fontweight'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4b1650-b94d-4cf6-860d-0f1092a18987",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df['Embarked'] = train_df['Embarked']\n",
    "train_df = pd.get_dummies(train_df, columns = ['Embarked'], drop_first = False)\n",
    "test_df = pd.get_dummies(test_df, columns = ['Embarked'], drop_first = False)\n",
    "df_list = [train_df, test_df]\n",
    "\n",
    "for df in df_list:\n",
    "    for c in [c for c in df.columns if 'Embarked' in c]:\n",
    "        df[c] = df[c].astype(int)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349172dd-370d-416f-ba9f-b08c950e8681",
   "metadata": {},
   "source": [
    "# 3. Correlation Matrix\n",
    "\n",
    "Decided to wait after hot encoding to better understand how to impute missing features.\n",
    "\n",
    "Features with missing values:\n",
    "1. **Age:** Correlated with Class and Titles (Master, Miss, Mr, Mrs, and Rare)\n",
    "2. **Fare:** Correlated with Class, Port of Embarkation, and Ticket (P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c27a7f6-987c-4f57-a566-44ad01eb9578",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize = (18, 10))\n",
    "corr_df = train_df.drop('PassengerId', axis = 1).corr(numeric_only = True).abs()\n",
    "sns.heatmap(corr_df, cmap = \"PiYG\", annot = True, annot_kws = {\"fontsize\": 6})\n",
    "plt.title('Correlation Matrix', fontdict = plot_title_dict)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b7771b-9fc7-43c9-9e41-074d02b59036",
   "metadata": {},
   "source": [
    "# 4. Feature Engineering, Impute, Bin Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f32486-16b8-477b-abe5-808cc1e6b3a5",
   "metadata": {},
   "source": [
    "# 4.1 Age\n",
    "1. Correlated with Class and Titles (Master, Miss, Mr, Mrs, and Rare)\n",
    "2. Impute based on correlation\n",
    "3. Add indicator for missing ages\n",
    "4. Bin Ages if model perfoms better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1956868-596d-4592-878d-79e800d9d16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data = train_df, \n",
    "            x = 'Age', \n",
    "            hue = 'Survived', \n",
    "            kde = True,\n",
    "            height = 5,\n",
    "            aspect = 2,\n",
    "            multiple = 'stack')\n",
    "plt.suptitle('Distribution of Age by Survival',\n",
    "             fontsize = plot_title_dict['fontsize'],\n",
    "             fontweight = plot_title_dict['fontweight'])\n",
    "plt.ylabel('Count', fontdict = plot_axis_dict)\n",
    "plt.xlabel('Age', fontdict = plot_axis_dict)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c8b16b-f630-4a5c-bde4-49479dee7f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize = (12, 6))\n",
    "sns.violinplot(data = plot_df, x = \"Pclass\", y = \"Age\", hue = \"Survived\", split = True, ax = ax)\n",
    "ax.set_title('Age Distribution by Class and Survival', fontdict = plot_title_dict)\n",
    "plt.ylabel('Age', fontdict = plot_axis_dict)\n",
    "plt.xlabel('Pclass', fontdict = plot_axis_dict)\n",
    "plt.grid(axis = 'y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e28e66c-4c78-4d72-9e3d-e35df24434b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_age_cols = ['Pclass', 'Title_Master', 'Title_Mr', 'Title_Mrs', 'Title_Miss', 'Title_Rare']\n",
    "age_missing_df = train_df[missing_age_cols + ['Age']]\\\n",
    "    .dropna()\\\n",
    "    .groupby(missing_age_cols, as_index = False)\\\n",
    "    .median()\n",
    "age_missing_df.columns = age_missing_df.columns.str.replace('Age', 'Age_Median')\n",
    "age_missing_df['Age_Median'] = age_missing_df['Age_Median'].astype(int)\n",
    "\n",
    "age_missing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67f9972-06ba-4526-97c0-9330a69fb7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in df_list:\n",
    "    df.loc[df['Age'].isnull(), 'Age_Missing'] = 1\n",
    "    df['Age_Missing'] = df['Age_Missing'].fillna(0).astype(int)\n",
    "    df['Age_Median'] = df.merge(age_missing_df, on = missing_age_cols, how = 'left')['Age_Median']\n",
    "    df.loc[df['Age'].isnull(), 'Age'] = df['Age_Median']\n",
    "    df.drop(['Age_Median'], axis = 1, inplace = True)\n",
    "    df['Age'] = df['Age'].astype(int)\n",
    "plot_df['Age'] = train_df['Age']\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101a5888-d9a7-4ca8-9241-9e4641f55e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_bins = 9\n",
    "# train_df['Age_Interval'] = pd.qcut(train_df['Age'], num_bins)\n",
    "\n",
    "# age_int_df = train_df\\\n",
    "#     .groupby(['Age_Interval'], as_index = False)\\\n",
    "#     .agg(Survived = ('Survived', 'mean'),\n",
    "#          Count = ('Age_Interval', 'count'))\n",
    "# age_int_df['Age'] = range(num_bins)\n",
    "\n",
    "# fig, axs = plt.subplots(figsize = (22, 9))\n",
    "# sns.countplot(x = 'Age_Interval', hue = 'Survived', data = train_df)\n",
    "# plt.title('Distribution of Passengers By Age Interval and Survival')\n",
    "# plt.show()\n",
    "\n",
    "# age_int_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc82266-44f7-446b-a50b-93621495c8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for df in df_list:\n",
    "#     df['Age'] = [[b for i, b in zip(age_int_df['Age_Interval'], age_int_df['Age']) if a in i][0] for a in df['Age']]\n",
    "# train_df.drop(['Age_Interval'], axis = 1, inplace = True)\n",
    "\n",
    "# train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f8c3d5-183e-4bc4-b4f9-c2601fc37b26",
   "metadata": {},
   "source": [
    "# 4.2 Passenger Class\n",
    "1. One hot encode pclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826899d6-1377-4717-92f4-1da227ece885",
   "metadata": {},
   "outputs": [],
   "source": [
    "surv_percent_fun('Pclass', (14, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a93a6a4-5d96-41a8-9c5f-1908dbeea499",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize = (12, 6))\n",
    "sns.barplot(x = 'Pclass', y = 'Survived', hue = 'Sex', data = plot_df)\n",
    "plt.title('Survival Rates by Gender and Pclass', fontdict = plot_title_dict)\n",
    "plt.ylabel('Survival Rate', fontdict = plot_axis_dict)\n",
    "plt.xlabel('Pclass', fontdict = plot_axis_dict)\n",
    "ax.yaxis.set_major_formatter(mtick.PercentFormatter(xmax = 1))\n",
    "plt.grid(axis = 'y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26d2a11-8a32-446f-9130-9082197e2182",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.get_dummies(train_df, columns = ['Pclass'], drop_first = False)\n",
    "test_df = pd.get_dummies(test_df, columns = ['Pclass'], drop_first = False)\n",
    "df_list = [train_df, test_df]\n",
    "\n",
    "for df in df_list:\n",
    "    for c in [c for c in df.columns if 'Pclass' in c]:\n",
    "        df[c] = df[c].astype(int)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb1d000-326e-48c8-83c9-40f9d0425ce5",
   "metadata": {},
   "source": [
    "# 4.3 Fare\n",
    "1. Correlated with Class, Port of Embarkation, and Ticket (P)\n",
    "2. Impute based on correlation\n",
    "3. Add indicator for missing fare\n",
    "4. Bin Fares if model perfoms better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31f5be8-5288-4e83-8d36-517d8f56b6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(data = train_df, \n",
    "            x = 'Fare', \n",
    "            hue = 'Survived', \n",
    "            kde = True,\n",
    "            height = 5,\n",
    "            aspect = 2,\n",
    "            multiple = 'stack')\n",
    "plt.suptitle('Distribution of Fare by Survival',\n",
    "             fontsize = plot_title_dict['fontsize'],\n",
    "             fontweight = plot_title_dict['fontweight'])\n",
    "plt.ylabel('Count', fontdict = plot_axis_dict)\n",
    "plt.xlabel('Fare', fontdict = plot_axis_dict)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ec2ef9-5d33-4f5f-8836-f03d2aa6b2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_fare_cols = ['Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S', 'Ticket_P']\n",
    "fare_missing_df = train_df[missing_fare_cols + ['Fare']]\\\n",
    "    .dropna()\\\n",
    "    .groupby(missing_fare_cols, as_index = False)\\\n",
    "    .median()\n",
    "fare_missing_df.columns = fare_missing_df.columns.str.replace('Fare', 'Fare_Median')\n",
    "\n",
    "fare_missing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3a27ea-dac4-4337-a641-56650d7f9483",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in df_list:\n",
    "    df.loc[df['Fare'].isnull(), 'Fare_Missing'] = 1\n",
    "    df['Fare_Missing'] = df['Fare_Missing'].fillna(0).astype(int)\n",
    "    df['Fare_Median'] = df.merge(fare_missing_df, on = missing_fare_cols, how = 'left')['Fare_Median']\n",
    "    df.loc[df['Fare'].isnull(), 'Fare'] = df['Fare_Median']\n",
    "    df.drop(['Fare_Median'], axis = 1, inplace = True)\n",
    "\n",
    "plot_df['Fare'] = train_df['Fare']\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25108ce-cb52-4bba-ae16-75b888d748d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for df in df_list:\n",
    "#     df['Fare_Log'] = np.log(df['Fare'])\n",
    "#     df.loc[df['Fare_Log'].isin([np.inf, -np.inf]), 'Fare_Log'] = 0\n",
    "#     df.drop(['Fare'], axis = 1, inplace = True)\n",
    "\n",
    "# plot_df['Fare_Log'] = train_df['Fare_Log']\n",
    "\n",
    "# train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8c94a3-e31a-42a5-9e5a-dd7f837cbcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.displot(data = train_df, \n",
    "#             x = 'Fare_Log', \n",
    "#             hue = 'Survived', \n",
    "#             kde = True,\n",
    "#             height = 5,\n",
    "#             aspect = 2,\n",
    "#             multiple = 'stack')\n",
    "# plt.suptitle('Distribution of Fare (Log) by Survival')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cf0e84-e55d-4528-b378-3c5d892ac543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_bins = 10\n",
    "# train_df['Fare_Interval'] = pd.qcut(train_df['Fare'], num_bins, precision = 4)\n",
    "\n",
    "# fare_int_df = train_df\\\n",
    "#     .groupby(['Fare_Interval'], as_index = False)\\\n",
    "#     .agg(Survived = ('Survived', 'mean'),\n",
    "#          Count = ('Fare_Interval', 'count'))\n",
    "# fare_int_df['Fare'] = range(num_bins)\n",
    "\n",
    "# fig, axs = plt.subplots(figsize = (22, 9))\n",
    "# sns.countplot(x = 'Fare_Interval', hue = 'Survived', data = train_df)\n",
    "# plt.title('Distribution of Passengers By Fare Interval and Survival')\n",
    "# plt.show()\n",
    "\n",
    "# fare_int_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9531b79-35e6-4999-a04c-04b4cb933c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for df in df_list:\n",
    "#     df['Fare'] = [[b for i, b in zip(fare_int_df['Fare_Interval'], fare_int_df['Fare']) if a in i][0] for a in df['Fare']]\n",
    "# train_df.drop(['Fare_Interval'], axis = 1, inplace = True)\n",
    "\n",
    "# train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b755ce3c-1cfe-4268-bd04-b0d71c6fd8d9",
   "metadata": {},
   "source": [
    "# 4.4 Family Size = SibSp + Parch\n",
    "1. Create Family Size\n",
    "2. Drop SibSp, and Parch\n",
    "3. Bin Fares if model perfoms better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1649ee-e3bf-44f6-8483-bb14a807402d",
   "metadata": {},
   "outputs": [],
   "source": [
    "surv_percent_fun('Parch', (14, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f3cc52-0f3e-4b27-afd2-f41808f1c0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "surv_percent_fun('SibSp', (14, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13cfc04-ed1d-4fad-8515-0df55f877c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in df_list:\n",
    "    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
    "    df.drop(['Parch', 'SibSp'], axis = 1, inplace = True)\n",
    "\n",
    "plot_df['FamilySize'] = train_df['FamilySize']\n",
    "\n",
    "surv_percent_fun('FamilySize', (14, 6))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656b0e1b-b610-48ac-84eb-83f532ba3a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for df in df_list:    \n",
    "#     df.loc[ df['FamilySize'] <= 1, 'FamilySizeBin'] = 'Alone'\n",
    "#     df.loc[(df['FamilySize'] > 1) & (df['FamilySize'] <= 4), 'FamilySizeBin'] = 'Small'\n",
    "#     df.loc[(df['FamilySize'] > 4) & (df['FamilySize'] <= 6), 'FamilySizeBin'] = 'Medium'\n",
    "#     df.loc[df['FamilySize'] > 6, 'FamilySizeBin'] = 'Large'\n",
    "#     df.drop(['FamilySize'], axis = 1, inplace = True)\n",
    "    \n",
    "# train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e294e6a5-afd8-4b01-ba23-91e66acbc977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f, ax = plt.subplots(figsize = (12, 6))\n",
    "# sns.barplot(x = 'FamilySizeBin', y = 'Survived', data = train_df)\n",
    "# plt.title('Survival Rates by Family Size')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6145e3-1161-49f0-9517-75b22a7b6754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = pd.get_dummies(train_df, columns = ['FamilySizeBin'], drop_first = False)\n",
    "# test_df = pd.get_dummies(test_df, columns = ['FamilySizeBin'], drop_first = False)\n",
    "# df_list = [train_df, test_df]\n",
    "\n",
    "# for df in df_list:\n",
    "#     for c in [c for c in df.columns if 'FamilySizeBin' in c]:\n",
    "#         df[c] = df[c].astype(int)\n",
    "#     df.columns = df.columns.str.replace('FamilySizeBin', 'Fam_Size')\n",
    "\n",
    "# train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15e2a80-e99b-4d5b-a6b5-7f9a375186b3",
   "metadata": {},
   "source": [
    "# 4.5 Cabin\n",
    "1. Extract Deck from Cabin: First Cabin letter is the Deck\n",
    "2. Do not impute missing cabin; group together as Deck X\n",
    "3. Drop Cabin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f5dab2-50ab-4483-b4e2-b5391c853a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in df_list:\n",
    "    df['Deck'] = [re.sub(r'\\d+', '', c.split()[0])for c in df['Cabin'].fillna('X')]\n",
    "    df.loc[df['Deck'] == 'T', 'Deck'] = 'A'\n",
    "\n",
    "surv_percent_fun('Deck', (14, 6))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f6bcc3-7197-49f2-af89-ed6c4ec6143d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df['Deck'] = train_df['Deck']\n",
    "train_df = pd.get_dummies(train_df, columns = ['Deck'], drop_first = False)\n",
    "test_df = pd.get_dummies(test_df, columns = ['Deck'], drop_first = False)\n",
    "df_list = [train_df, test_df]\n",
    "\n",
    "for df in df_list:\n",
    "    for c in [c for c in df.columns if 'Deck' in c]:\n",
    "        df[c] = df[c].astype(int)\n",
    "    df.drop(['Cabin'], axis = 1, inplace = True)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e9f1bb-8d1a-47bd-bb73-0e2ff8028768",
   "metadata": {},
   "source": [
    "# 4.6 Passenger ID\n",
    "1. Drop from training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbb0939-89dc-4c0d-9055-39dce8408a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(['PassengerId'], axis = 1, inplace = True)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92fc89d-73e0-4e84-ab6c-831e75341341",
   "metadata": {},
   "source": [
    "# 5. Add Interactions\n",
    "\n",
    "Explore interactions between Fare, Age, and Family Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02d4354-371c-4986-ad43-7722d3cc0b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in df_list:\n",
    "    # df['FS*Fare'] = df['FamilySize'] * df['Fare']\n",
    "    # df['FS*Age'] = df['FamilySize'] * df['Age']\n",
    "    df['Female*Age'] = df['Sex_Female'] * df['Age']\n",
    "    df['Male*Fare'] = df['Sex_Male'] * df['Fare']\n",
    "    df['Fare*Age'] = df['Fare'] * df['Age']\n",
    "    \n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca08894d-e41d-46bc-a5c9-1153c73477f1",
   "metadata": {},
   "source": [
    "# 6. Score Models\n",
    "1. Final check for missing features\n",
    "2. Scale features\n",
    "3. Train candidate models and store in dictionary\n",
    "4. Rank candidate models by cross-validated accuracy \n",
    "\n",
    "**Candidate Models:**\n",
    "1. Logistic Regression\n",
    "2. Support Vector Machines\n",
    "3. KNN\n",
    "4. Naive Bayes\n",
    "5. Perceptron\n",
    "6. Linear SVC\n",
    "7. Stochastic Gradient Decent\n",
    "8. Decision Tree\n",
    "9. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dbcd61-349c-4868-9ae2-71079c3b6908",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ffc4d4-1ddb-41b4-9146-02b9ab195c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([train_df, test_df]).isnull().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3062d79c-f251-4aa9-b4e7-39f5b5e9acbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split X, Y\n",
    "X_train = train_df.drop(\"Survived\", axis = 1)\n",
    "Y_train = train_df[\"Survived\"]\n",
    "X_test  = test_df.drop(\"PassengerId\", axis = 1).copy()\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108a9c69-eab2-40df-9331-e73be265da50",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "svc = SVC(probability = True)\n",
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "gaussian = GaussianNB()\n",
    "perceptron = Perceptron()\n",
    "linear_svc = LinearSVC()\n",
    "sgd = SGDClassifier()\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "random_forest = RandomForestClassifier(n_estimators = 100)\n",
    "xgb_boost = XGBClassifier(n_estimators = 100)\n",
    "\n",
    "model_dict = {'Logistic Regression': {'Model': logreg, 'X_Train': X_train_scaled, 'X_Test': X_test_scaled},\n",
    "              'Support Vector Machines': {'Model': svc, 'X_Train': X_train_scaled, 'X_Test': X_test_scaled},\n",
    "              'KNN': {'Model': knn, 'X_Train': X_train_scaled, 'X_Test': X_test_scaled},\n",
    "              'Naive Bayes': {'Model': gaussian, 'X_Train': X_train, 'X_Test': X_test},\n",
    "              'Perceptron': {'Model': perceptron, 'X_Train': X_train_scaled, 'X_Test': X_test_scaled},\n",
    "              'Linear SVC': {'Model': linear_svc, 'X_Train': X_train_scaled, 'X_Test': X_test_scaled},\n",
    "              'Stochastic Gradient Decent': {'Model': sgd, 'X_Train': X_train_scaled, 'X_Test': X_test_scaled},\n",
    "              'Decision Tree': {'Model': decision_tree, 'X_Train': X_train, 'X_Test': X_test},\n",
    "              'Random Forest': {'Model': random_forest, 'X_Train': X_train_scaled, 'X_Test': X_test_scaled},\n",
    "              'XGBoost': {'Model': xgb_boost, 'X_Train': X_train, 'X_Test': X_test}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008a7986-d60f-418c-a066-a4d78329d8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_df = pd.DataFrame()\n",
    "\n",
    "for d in model_dict:\n",
    "    print('Training ' + d)\n",
    "    \n",
    "    # Fit Models\n",
    "    model_dict[d]['Model'].fit(model_dict[d]['X_Train'], Y_train)\n",
    "    \n",
    "    # Store Probabilities (not all models support predict_proba)\n",
    "    # Only need this in case model is later included in ensemble\n",
    "    if d in ['Logistic Regression', 'Support Vector Machines', 'KNN', 'Naive Bayes', \n",
    "             'Decision Tree', 'Random Forest', 'XGBoost']:\n",
    "        model_dict[d]['CV_Prob'] = cross_val_predict(model_dict[d]['Model'],\n",
    "                                                     model_dict[d]['X_Train'],\n",
    "                                                     Y_train,\n",
    "                                                     cv = 10,\n",
    "                                                     method = 'predict_proba')[:,1]\n",
    "        model_dict[d]['Train_Prob'] = model_dict[d]['Model'].predict_proba(model_dict[d]['X_Train'])[:,1]\n",
    "        model_dict[d]['Test_Prob'] = model_dict[d]['Model'].predict_proba(model_dict[d]['X_Test'])[:,1]\n",
    "    \n",
    "    # Calculate CV Accuracy and AUC\n",
    "    cv_scores_acc = cross_val_score(model_dict[d]['Model'], \n",
    "                                    model_dict[d]['X_Train'],\n",
    "                                    Y_train, \n",
    "                                    cv = 10, \n",
    "                                    scoring = \"accuracy\")\n",
    "    cv_scores_auc = cross_val_score(model_dict[d]['Model'], \n",
    "                                    model_dict[d]['X_Train'],\n",
    "                                    Y_train, \n",
    "                                    cv = 10, \n",
    "                                    scoring = \"roc_auc\")\n",
    "    model_score = model_dict[d]['Model'].score(model_dict[d]['X_Train'], Y_train)\n",
    "\n",
    "    # Append metrics\n",
    "    df = pd.DataFrame({'Model': [d],\n",
    "                       'Score': [round(model_score * 100, 2)],\n",
    "                       'CV_Accuracy_Mean': [round(cv_scores_acc.mean() * 100, 2)],\n",
    "                       'CV_Accuracy_SD': [round(cv_scores_acc.std() * 100, 2)],\n",
    "                       'CV_ROC_AUC_Mean': [round(cv_scores_auc.mean() * 100, 2)],\n",
    "                       'CV_ROC_AUC_SD': [round(cv_scores_auc.std() * 100, 2)]})\n",
    "    models_df = pd.concat([models_df, df]).reset_index(drop = True)\n",
    "\n",
    "models_df.sort_values(by = 'CV_Accuracy_Mean', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dc5ef3-174a-49af-ad06-431cf3ce1830",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_pickle('data/baseline').sort_values(by = 'CV_Accuracy_Mean', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7c9d14-f969-4a26-baf1-c4d2df62aae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save as baseline if results favorable\n",
    "# models_df.to_pickle('data/baseline')\n",
    "# train_df.to_pickle('data/train')\n",
    "# test_df.to_pickle('data/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef721c60-59e0-4aed-8d85-51c2743e120f",
   "metadata": {},
   "source": [
    "# 7. Tune Hyper Parameters\n",
    "\n",
    "1. Create model objective function with paramater space\n",
    "2. Create and run the optimization process with 100 trials\n",
    "3. Retrieve the best parameter values and fine tune model\n",
    "4. Train model with best parameter and store in dictionary\n",
    "5. Evaluate feature importance\n",
    "\n",
    "References:\n",
    "\n",
    "1. https://www.youtube.com/watch?v=D9xPjkOwpNk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c17fb1-110d-4001-b4b7-4ab7f166f09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c1da05-0500-4331-9bcb-7aed18c77a21",
   "metadata": {},
   "source": [
    "# 7.1 Random Forest Hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55716253-a7de-40f3-9f35-576cb01d6e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict['Random Forest Hyper Tuned'] = {'X_Train': X_train_scaled, 'X_Test': X_test_scaled}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfcdb04-0437-45e8-9a2c-3a7da95853a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_obj_fun(trial):\n",
    "    param = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 30),\n",
    "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 20),\n",
    "        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 10)\n",
    "    }\n",
    "\n",
    "    model = RandomForestClassifier(**param,)\n",
    "    score = cross_val_score(model, \n",
    "                            model_dict['Random Forest Hyper Tuned']['X_Train'], \n",
    "                            Y_train, cv = 3, \n",
    "                            scoring = \"roc_auc\").mean()\n",
    "    return score\n",
    "\n",
    "rf_study = optuna.create_study(study_name = 'rf_study_cpu', direction = \"maximize\")\n",
    "rf_study.optimize(rf_obj_fun,\n",
    "                  n_trials = 10,\n",
    "                  show_progress_bar = True,\n",
    "                  n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1f1692-9720-4c9d-9724-0b77fdd2eecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best_params = rf_study.best_params\n",
    "print(f\"\\nBest parameters: {rf_best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e028646-e6e5-4efd-97d3-2f0096a7249f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_tuned = RandomForestClassifier(n_estimators = rf_best_params['n_estimators'],\n",
    "                                  max_depth = rf_best_params['max_depth'],\n",
    "                                  min_samples_split = rf_best_params['min_samples_split'],\n",
    "                                  min_samples_leaf = rf_best_params['min_samples_leaf'])\n",
    "rf_tuned.fit(model_dict['Random Forest Hyper Tuned']['X_Train'], Y_train)\n",
    "model_dict['Random Forest Hyper Tuned']['Model'] = rf_tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6575b60-6fbb-47e7-a467-08c52ec6c32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_feature_imp = pd.DataFrame({'feature': X_train.columns, \n",
    "                               'importance': np.round(model_dict['Random Forest Hyper Tuned']['Model'].feature_importances_,3)})\n",
    "rf_feature_imp = rf_feature_imp.sort_values('importance', ascending = False).set_index('feature')\n",
    "\n",
    "f, ax = plt.subplots(figsize = (12, 6))\n",
    "rf_feature_imp.plot.bar(ax = ax)\n",
    "plt.title('Random Forest Feature Importance', fontdict = plot_title_dict)\n",
    "plt.ylabel('Importance', fontdict = plot_axis_dict)\n",
    "plt.xlabel('Feature', fontdict = plot_axis_dict)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cccc007-ad30-4a0f-a41a-6e8fbe2c1f2c",
   "metadata": {},
   "source": [
    "# 7.2 XGBoost Hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1a568f-d849-45eb-9be4-5d08c3176e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict['XGBoost Hyper Tuned'] = {'X_Train': X_train_scaled, 'X_Test': X_test_scaled}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b394776c-b987-47bf-8c58-57a279f16f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_obj_fun(trial):\n",
    "    param = {\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.1),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0, 5),\n",
    "    }\n",
    "    \n",
    "    model = xgb.XGBClassifier(**param)\n",
    "    score = cross_val_score(model, \n",
    "                            model_dict['XGBoost Hyper Tuned']['X_Train'], \n",
    "                            Y_train, cv = 3, \n",
    "                            scoring = \"roc_auc\").mean()\n",
    "    return score\n",
    "\n",
    "xgb_study = optuna.create_study(study_name = 'xgboost_study_cpu', direction = \"maximize\")\n",
    "xgb_study.optimize(xgb_obj_fun,\n",
    "                   n_trials = 10,\n",
    "                   show_progress_bar = True,\n",
    "                   n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd3f711-2ef5-48b6-913c-b2959367cf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_best_params = xgb_study.best_params\n",
    "print(f\"\\nBest parameters: {xgb_best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec39b03-f902-4300-8dd7-2d920e6a4a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_tuned = XGBClassifier(n_estimators = xgb_best_params['n_estimators'],\n",
    "                          max_depth = xgb_best_params['max_depth'],\n",
    "                          learning_rate = xgb_best_params['learning_rate'],\n",
    "                          subsample = xgb_best_params['subsample'],\n",
    "                          colsample_bytree = xgb_best_params['colsample_bytree'],\n",
    "                          min_child_weight = xgb_best_params['min_child_weight'],\n",
    "                          gamma = xgb_best_params['gamma'])\n",
    "xgb_tuned.fit(model_dict['XGBoost Hyper Tuned']['X_Train'], Y_train)\n",
    "model_dict['XGBoost Hyper Tuned']['Model'] = xgb_tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbfa982-0d16-4853-b55a-7b5440e8d083",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_feature_imp = pd.DataFrame({'feature': X_train.columns, \n",
    "                                'importance': np.round(model_dict['XGBoost Hyper Tuned']['Model'].feature_importances_,3)})\n",
    "xgb_feature_imp = xgb_feature_imp.sort_values('importance', ascending = False).set_index('feature')\n",
    "\n",
    "f, ax = plt.subplots(figsize = (12, 6))\n",
    "xgb_feature_imp.plot.bar(ax = ax)\n",
    "plt.title('XGBoost Feature Importance', fontdict = plot_title_dict)\n",
    "plt.ylabel('Importance', fontdict = plot_axis_dict)\n",
    "plt.xlabel('Feature', fontdict = plot_axis_dict)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b542d7-0a01-4210-8cb1-b908e2f88977",
   "metadata": {},
   "source": [
    "# 8. Calculate Model Probabilities\n",
    "\n",
    "1. Narrow down initial list of models with best candidates\n",
    "2. Calculate and store model probabilities\n",
    "3. Evaluate Metrics\n",
    "\n",
    "Metrics:\n",
    "1. Confusion Matrix\n",
    "2. Precision and Recall\n",
    "3. F1 Score\n",
    "4. ROC AUC Curve and Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b4679d-6512-4a6d-84ce-94fe8d654544",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, f1_score, roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256a1bda-5cfe-47b4-9705-42986b90071a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_options = ['Logistic Regression',\n",
    "                 'Support Vector Machines',\n",
    "                 'Random Forest Hyper Tuned',\n",
    "                 'XGBoost Hyper Tuned']\n",
    "\n",
    "for final_model in final_options:\n",
    "    model_dict[final_model]['CV_Prob'] = cross_val_predict(model_dict[final_model]['Model'],\n",
    "                                                           model_dict[final_model]['X_Train'],\n",
    "                                                           Y_train,\n",
    "                                                           cv = 10,\n",
    "                                                           method = 'predict_proba')[:,1]\n",
    "    model_dict[final_model]['Train_Prob'] = model_dict[final_model]['Model'].predict_proba(model_dict[final_model]['X_Train'])[:,1]\n",
    "    model_dict[final_model]['Test_Prob'] = model_dict[final_model]['Model'].predict_proba(model_dict[final_model]['X_Test'])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e2a435-7141-43cf-a362-9e44f1fe739e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_metrics_df = pd.DataFrame()\n",
    "fpr_list = []\n",
    "tpr_list = []\n",
    "conf_mat_list = []\n",
    "roc_auc_list = []\n",
    "for final_model in final_options:\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cv_preds = [int(float(p) > 0.5) for p in model_dict[final_model]['CV_Prob']]\n",
    "    model_confusion_matrix = confusion_matrix(Y_train, cv_preds)\n",
    "    conf_mat_list.append(model_confusion_matrix)\n",
    "    true_non_surv = model_confusion_matrix[0][0]\n",
    "    true_surv = model_confusion_matrix[1][1]\n",
    "\n",
    "    # ROC Curve\n",
    "    y_train_prob = model_dict[final_model]['Train_Prob']\n",
    "    model_fpr, model_tpr, model_thresholds = roc_curve(Y_train, y_train_prob)\n",
    "    fpr_list.append(model_fpr)\n",
    "    tpr_list.append(model_tpr)\n",
    "    model_roc_auc = roc_auc_score(Y_train, y_train_prob)\n",
    "    roc_auc_list.append(model_roc_auc)\n",
    "\n",
    "    # Metrics\n",
    "    model_precision = precision_score(Y_train, cv_preds)\n",
    "    model_recall = recall_score(Y_train, cv_preds)\n",
    "    model_f1 = f1_score(Y_train, cv_preds)\n",
    "\n",
    "    final_metrics_df = pd.concat([final_metrics_df,\n",
    "                                  pd.DataFrame({'Selected Model': [final_model],\n",
    "                                                'Precision': [round(model_precision * 100, 2)],\n",
    "                                                'Recall': [round(model_recall * 100, 2)],\n",
    "                                                'F1 Score': [round(model_f1 * 100, 2)],\n",
    "                                                'ROC AUC': [round(model_roc_auc * 100, 2)], \n",
    "                                                'Non Survivors': true_non_surv,\n",
    "                                                'Survivors': true_surv,\n",
    "                                                'Total Correct': true_non_surv + true_surv\n",
    "                                               })]).reset_index(drop = True)\n",
    "final_metrics_df.sort_values(by = 'Precision', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cf54e5-22e8-409b-8bf1-83f1466fbcda",
   "metadata": {},
   "source": [
    "# 9. Create Ensemble Models\n",
    "\n",
    "SVC good at predicting true non survivors, but other models good at predicting survivors. Explore ensembles\n",
    "\n",
    "Cannot include Perceptron, Linear SVC, Stochastic Gradient Decent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1d80af-ca06-4859-84ff-0e3a8595476e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble1_name = 'Ensemble: SVC + XGBoost Tuned'\n",
    "ensemble1 = ['Support Vector Machines', 'XGBoost Hyper Tuned']\n",
    "ensemble1_cv_prob = []\n",
    "ensemble1_train_prob = []\n",
    "ensemble1_test_prob = []\n",
    "for m in ensemble1:\n",
    "    ensemble1_cv_prob.append(model_dict[m]['CV_Prob'])\n",
    "    ensemble1_train_prob.append(model_dict[m]['Train_Prob'])\n",
    "    ensemble1_test_prob.append(model_dict[m]['Test_Prob'])\n",
    "model_dict[ensemble1_name] = {'CV_Prob': np.mean(ensemble1_cv_prob, axis = 0),\n",
    "                              'Train_Prob': np.mean(ensemble1_train_prob, axis = 0),\n",
    "                              'Test_Prob': np.mean(ensemble1_test_prob, axis = 0)}\n",
    "final_options = final_options + [ensemble1_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eafa586-99f1-4729-8600-f8cedc435b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble2_name = 'Ensemble: SVC + Random Forest Tuned'\n",
    "ensemble2 = ['Support Vector Machines', 'Random Forest Hyper Tuned']\n",
    "ensemble2_cv_prob = []\n",
    "ensemble2_train_prob = []\n",
    "ensemble2_test_prob = []\n",
    "for m in ensemble2:\n",
    "    ensemble2_cv_prob.append(model_dict[m]['CV_Prob'])\n",
    "    ensemble2_train_prob.append(model_dict[m]['Train_Prob'])\n",
    "    ensemble2_test_prob.append(model_dict[m]['Test_Prob'])\n",
    "model_dict[ensemble2_name] = {'CV_Prob': np.mean(ensemble2_cv_prob, axis = 0),\n",
    "                              'Train_Prob': np.mean(ensemble2_train_prob, axis = 0),\n",
    "                              'Test_Prob': np.mean(ensemble2_test_prob, axis = 0)}\n",
    "final_options = final_options + [ensemble2_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d80b82-0d22-4d07-b4d5-03408ef3abe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble3_name = 'Ensemble: SVC + RF Tuned + XGBoost Tuned'\n",
    "ensemble3 = ['Support Vector Machines', 'Random Forest Hyper Tuned', 'XGBoost Hyper Tuned']\n",
    "ensemble3_cv_prob = []\n",
    "ensemble3_train_prob = []\n",
    "ensemble3_test_prob = []\n",
    "for m in ensemble3:\n",
    "    ensemble3_cv_prob.append(model_dict[m]['CV_Prob'])\n",
    "    ensemble3_train_prob.append(model_dict[m]['Train_Prob'])\n",
    "    ensemble3_test_prob.append(model_dict[m]['Test_Prob'])\n",
    "model_dict[ensemble3_name] = {'CV_Prob': np.mean(ensemble3_cv_prob, axis = 0),\n",
    "                              'Train_Prob': np.mean(ensemble3_train_prob, axis = 0),\n",
    "                              'Test_Prob': np.mean(ensemble3_test_prob, axis = 0)}\n",
    "final_options = final_options + [ensemble3_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320fefe4-6502-43c5-9a55-0711ccb2b47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble4_name = 'Ensemble: SVC + Logistic Regression'\n",
    "ensemble4 = ['Support Vector Machines', 'Logistic Regression']\n",
    "ensemble4_cv_prob = []\n",
    "ensemble4_train_prob = []\n",
    "ensemble4_test_prob = []\n",
    "for m in ensemble3:\n",
    "    ensemble4_cv_prob.append(model_dict[m]['CV_Prob'])\n",
    "    ensemble4_train_prob.append(model_dict[m]['Train_Prob'])\n",
    "    ensemble4_test_prob.append(model_dict[m]['Test_Prob'])\n",
    "model_dict[ensemble4_name] = {'CV_Prob': np.mean(ensemble4_cv_prob, axis = 0),\n",
    "                              'Train_Prob': np.mean(ensemble4_train_prob, axis = 0),\n",
    "                              'Test_Prob': np.mean(ensemble4_test_prob, axis = 0)}\n",
    "final_options = final_options + [ensemble4_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d071a97f-68b4-4715-9618-d8b253402e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble5_name = 'Ensemble: SVC + Random Forest (Base)'\n",
    "ensemble5 = ['Support Vector Machines', 'Random Forest']\n",
    "ensemble5_cv_prob = []\n",
    "ensemble5_train_prob = []\n",
    "ensemble5_test_prob = []\n",
    "for m in ensemble5:\n",
    "    ensemble5_cv_prob.append(model_dict[m]['CV_Prob'])\n",
    "    ensemble5_train_prob.append(model_dict[m]['Train_Prob'])\n",
    "    ensemble5_test_prob.append(model_dict[m]['Test_Prob'])\n",
    "model_dict[ensemble5_name] = {'CV_Prob': np.mean(ensemble5_cv_prob, axis = 0),\n",
    "                              'Train_Prob': np.mean(ensemble5_train_prob, axis = 0),\n",
    "                              'Test_Prob': np.mean(ensemble5_test_prob, axis = 0)}\n",
    "final_options = final_options + [ensemble5_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f41d306-6f5a-4d16-85c7-d48fce052fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaa281f-2ae4-46e4-9cd0-ad4ab22bdaf2",
   "metadata": {},
   "source": [
    "# 10. Calculate Model Performance Metrics\n",
    "\n",
    "Re-evaluate metrics with ensemble models.\n",
    "\n",
    "Metrics:\n",
    "1. Confusion Matrix\n",
    "2. Precision and Recall\n",
    "3. F1 Score\n",
    "4. ROC AUC Curve and Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbce3c7-e61d-461f-9048-64fadbe3afac",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_metrics_df = pd.DataFrame()\n",
    "fpr_list = []\n",
    "tpr_list = []\n",
    "conf_mat_list = []\n",
    "roc_auc_list = []\n",
    "for final_model in final_options:\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cv_preds = [int(float(p) > 0.5) for p in model_dict[final_model]['CV_Prob']]\n",
    "    model_confusion_matrix = confusion_matrix(Y_train, cv_preds)\n",
    "    conf_mat_list.append(model_confusion_matrix)\n",
    "    true_non_surv = model_confusion_matrix[0][0]\n",
    "    true_surv = model_confusion_matrix[1][1]\n",
    "\n",
    "    # ROC Curve\n",
    "    y_train_prob = model_dict[final_model]['Train_Prob']\n",
    "    model_fpr, model_tpr, model_thresholds = roc_curve(Y_train, y_train_prob)\n",
    "    fpr_list.append(model_fpr)\n",
    "    tpr_list.append(model_tpr)\n",
    "    model_roc_auc = roc_auc_score(Y_train, y_train_prob)\n",
    "    roc_auc_list.append(model_roc_auc)\n",
    "\n",
    "    # Metrics\n",
    "    model_precision = precision_score(Y_train, cv_preds)\n",
    "    model_recall = recall_score(Y_train, cv_preds)\n",
    "    model_f1 = f1_score(Y_train, cv_preds)\n",
    "\n",
    "    final_metrics_df = pd.concat([final_metrics_df,\n",
    "                                  pd.DataFrame({'Selected Model': [final_model],\n",
    "                                                'Precision': [round(model_precision * 100, 2)],\n",
    "                                                'Recall': [round(model_recall * 100, 2)],\n",
    "                                                'F1 Score': [round(model_f1 * 100, 2)],\n",
    "                                                'ROC AUC': [round(model_roc_auc * 100, 2)], \n",
    "                                                'Non Survivors': true_non_surv,\n",
    "                                                'Survivors': true_surv,\n",
    "                                                'Total Correct': true_non_surv + true_surv\n",
    "                                               })]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0033e18e-7809-4ec0-a230-773f503444c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Titanic: 81.1% Leader board Score Guaranteed notebook predicted ' + str(501 + 252))\n",
    "final_metrics_df.sort_values(by = 'Precision', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605b6099-cfc2-4b91-bd8f-863e18d015e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_pickle('data/final_metrics').sort_values(by = 'Precision', ascending = False)\n",
    "\n",
    "# # Save for comp\n",
    "# final_metrics_df.to_pickle('data/final_metrics')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c5f28a-64d6-438a-9bce-30640e4c678b",
   "metadata": {},
   "source": [
    "# 11. Select Final Model\n",
    "\n",
    "Combining SVC and Random Forest scores highest across both precision and recall and also has the highest F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47233fc8-42be-4452-bb6e-93f0f1130a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = 5\n",
    "\n",
    "final_model = final_options[selection]\n",
    "print('Selected Model: ' + final_model)\n",
    "Y_test_prob = model_dict[final_model]['Test_Prob']\n",
    "submission_df = pd.DataFrame({\"PassengerId\": test_df[\"PassengerId\"],\n",
    "                              \"Survived\": [int(float(p) > 0.5) for p in Y_test_prob]})\n",
    "submission_df.to_csv('/Users/andressanchez/Documents/DataScience/Titanic/results/submission.csv', index = False)\n",
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb011c4a-3d03-4ed2-9d7d-40bf97f40d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize = (12, 6))\n",
    "\n",
    "sns.heatmap(conf_mat_list[selection],\n",
    "            annot = True,\n",
    "            fmt = ',.0f',\n",
    "            cmap = 'binary',\n",
    "            linewidths = 1, \n",
    "            linecolor = 'black',\n",
    "            cbar_kws = {'format': mtick.StrMethodFormatter('{x:,.0f}')},\n",
    "            ax = ax)\n",
    "plt.title('Confusion Matrix', fontdict = plot_title_dict)\n",
    "plt.ylabel('Actual Survival', fontdict = plot_axis_dict)\n",
    "plt.xlabel('Predicted', fontdict = plot_axis_dict)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18eac459-0e90-4080-b145-8c7558ca4094",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize = (14, 6))\n",
    "plt.plot(fpr_list[selection], \n",
    "         tpr_list[selection], \n",
    "         linewidth = 3, label = None)\n",
    "plt.plot([0, 1], [0, 1], 'r', linewidth = 3)\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.title('ROC-AUC-Score: ' +\"{:.2%}\".format(roc_auc_list[selection]), fontdict = plot_title_dict)\n",
    "plt.xlabel('False Positive Rate (FPR)', fontdict = plot_axis_dict)\n",
    "plt.ylabel('True Positive Rate (TPR)', fontdict = plot_axis_dict)\n",
    "ax.yaxis.set_major_formatter(mtick.PercentFormatter(xmax = 1))\n",
    "ax.xaxis.set_major_formatter(mtick.PercentFormatter(xmax = 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ff27f0-fd88-465f-9ad7-99fd58e8077b",
   "metadata": {},
   "source": [
    "# 12. Submit to Kaggle\n",
    "\n",
    "References:\n",
    "1. https://www.kaggle.com/docs/api\n",
    "2. https://www.youtube.com/watch?v=gkEbaMgvLs8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a623dd2-c161-413a-bd78-1c23ae1431ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Selected Model: ' + final_model)\n",
    "print('Make sure to update comment on submission')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbee37ae-3476-4474-9cb0-3d45c143b180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle competitions submit -c titanic -f results/submission.csv -m svc_rft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ee2941-847d-49f5-9654-9f350f32cc48",
   "metadata": {},
   "source": [
    "# 13. Compare Public Score with Leaderboard\n",
    "\n",
    "Models:\n",
    "1. Support Vector Machines Public Score: **0.79186**\n",
    "2. Random Forest Hyper Tuned Public Score: **0.78229**\n",
    "3. XGB Hyper Tuned Public Score: **0.76794**\n",
    "3. Logistic Regression Public Score: **0.76794** scaled, **0.77033** unscaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5d3f9e-0099-46a8-8056-e0bc4dd8693c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# public_score = 0.79186\n",
    "\n",
    "# scores_df = pd.read_csv(kaggle_path + '/titanic-leaderboard/titanic-publicleaderboard-2025-11-28T01-56-53.csv')\n",
    "# scores_df = scores_df[(scores_df['Score'] >= 0.70) & (scores_df['Score'] <= 0.85)].reset_index(drop = True)\n",
    "\n",
    "# total_scores = scores_df.shape[0]\n",
    "# better_scores = scores_df[scores_df['Score'] > public_score].shape[0]\n",
    "# worse_scores = scores_df[scores_df['Score'] <= public_score].shape[0]\n",
    "# score_pct = better_scores / total_scores\n",
    "\n",
    "# print('Public score: ' + \"{:.3%}\".format(public_score))\n",
    "# print('Mean score: ' + \"{:.3%}\".format(scores_df['Score'].mean()))\n",
    "# print('Median score: ' + \"{:.3%}\".format(scores_df['Score'].median()))\n",
    "# print(\"{:.1%}\".format(better_scores / total_scores) + ' are better')\n",
    "# print(\"{:.1%}\".format(worse_scores / total_scores) + ' are equal or worse')\n",
    "# print('Out of ' + \"{:,}\".format(total_scores) + ' scores, ' + \"{:,}\".format(better_scores) + ' scored higher.')\n",
    "\n",
    "# sns.displot(data = scores_df, \n",
    "#             x = 'Score', \n",
    "#             kde = True,\n",
    "#             height = 5,\n",
    "#             aspect = 2)\n",
    "# plt.axvline(x = public_score, color = 'red')\n",
    "# plt.suptitle('Distribution of Kaggle Public Scores')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
